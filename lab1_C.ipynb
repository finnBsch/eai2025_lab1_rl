{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lab_c_header",
   "metadata": {},
   "source": [
    "# Lab 1 - Level C: Reward Tuning for Challenging Terrain\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this level, you will tune the reward function to train the quadruped robot to navigate more challenging environments, specifically to walk over elevated steps.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Successfully completed Level E (PPO loss function implementation)\n",
    "- Understanding of reward shaping in reinforcement learning\n",
    "- Completed setup instructions from main `README.md`\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Reward function design for complex locomotion tasks\n",
    "- How different reward components affect robot behavior\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "Your main tasks for Level C:\n",
    "\n",
    "1. **Environment Analysis**: Understand the challenging terrain environment\n",
    "2. **Reward Function Design**: Modify and tune reward components for step climbing\n",
    "3. **Training**: Train the robot with your improved reward function\n",
    "4. **Evaluation**: Test the trained policy\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "After successful completion, you should see:\n",
    "- Robot successfully navigating elevated steps\n",
    "- Stable locomotion on uneven terrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unset LD_LIBRARY_PATH\n",
    "import os\n",
    "if 'LD_LIBRARY_PATH' in os.environ:\n",
    "    os.environ['LD_LIBRARY_PATH'] = ''\n",
    "    print(\"LD_LIBRARY_PATH has been unset\")\n",
    "else:\n",
    "    print(\"LD_LIBRARY_PATH was not set\")\n",
    "# Set EGL vendor directory to include user location\n",
    "user_vendor_dir = os.path.expanduser('~/.local/share/glvnd/egl_vendor.d')\n",
    "os.makedirs(user_vendor_dir, exist_ok=True)\n",
    "# Create ICD config\n",
    "icd_config = {\n",
    "    \"file_format_version\": \"1.0.0\",\n",
    "    \"ICD\": {\n",
    "        \"library_path\": \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "import json\n",
    "with open(f'{user_vendor_dir}/10_nvidia.json', 'w') as f:\n",
    "    json.dump(icd_config, f, indent=2)\n",
    "# Set environment variable\n",
    "current_dirs = os.environ.get('__EGL_VENDOR_LIBRARY_DIRS', '/usr/share/glvnd/egl_vendor.d')\n",
    "os.environ['__EGL_VENDOR_LIBRARY_DIRS'] = f'{user_vendor_dir}:{current_dirs}'\n",
    "# Set MuJoCo to use EGL\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "print('Setting environment variable to use GPU rendering:')\n",
    "%env MUJOCO_GL=egl\n",
    "# Tell XLA to use Triton GEMM\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import functools\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from IPython.display import HTML, clear_output\n",
    "import mujoco\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "import cv2\n",
    "import custom_ppo_train\n",
    "from utils import render_video_during_training, evaluate_policy\n",
    "import mediapy as media\n",
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "#os.environ['MUJOCO_GL'] = 'egl'\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True   # Show visual geoms\n",
    "scene_option.geomgroup[3] = False  # Hide collision geoms\n",
    "scene_option.geomgroup[5] = True   # Show sites (including height scanner visualization)\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Show contact points\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_RANGEFINDER] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jax_devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JAX devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_challenging_env",
   "metadata": {},
   "source": [
    "## Load the Challenging Environment\n",
    "\n",
    "We'll load a more challenging environment with elevated steps and rough terrain that the robot needs to navigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Go1JoystickRoughTerrain'  # Update this to actual challenging environment\n",
    "env = registry.load(env_name)\n",
    "key = jax.random.PRNGKey(15)\n",
    "\n",
    "# Visualize the challenging environment\n",
    "num_resets = 10\n",
    "frames_per_reset = 50\n",
    "\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "rollout = []\n",
    "\n",
    "for reset_idx in range(num_resets):\n",
    "    key, reset_key = jax.random.split(key)\n",
    "    \n",
    "    # Reset to new random position\n",
    "    state = jit_reset(reset_key)\n",
    "    \n",
    "    # Add multiple frames of the same reset position\n",
    "    for frame_idx in range(frames_per_reset):\n",
    "        rollout.append(state)\n",
    "        # Take a few small steps to show terrain\n",
    "        if frame_idx < 10:\n",
    "            action = jp.zeros(env.action_size)\n",
    "            state = jit_step(state, action)\n",
    "\n",
    "render_every = 2  # Render every 3rd frame\n",
    "fps = 1.0 / env.dt / render_every\n",
    "traj = rollout[::render_every]\n",
    "\n",
    "frames = env.render(\n",
    "    traj,\n",
    "    camera=\"track\",  # Use tracking camera\n",
    "    scene_option=scene_option,\n",
    "    width=640,\n",
    "    height=480,\n",
    ")\n",
    "media.show_video(frames, fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reward_analysis",
   "metadata": {},
   "source": [
    "## Level C Task: Reward Function Analysis and Tuning\n",
    "\n",
    "### Current Challenge\n",
    "\n",
    "The flat terrain policy from Level E likely won't work well on challenging terrain with steps and obstacles. You need to:\n",
    "\n",
    "1. **Analyze the current reward function**\n",
    "2. **Identify what behaviors need to be encouraged/discouraged**\n",
    "3. **Design new reward components**\n",
    "4. **Tune reward weights and parameters**\n",
    "\n",
    "You can find more details on the reward function definitions here: https://github.com/finnBsch/mujoco_playground/blob/lab1_rl/mujoco_playground/_src/locomotion/go1/joystick.py\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "Tune the reward function weights that enables the robot to successfully navigate the challenging terrain. Consider both positive rewards (for desired behaviors) and negative rewards/penalties (for undesired behaviors). You will need to explain your choices for the reward weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_section",
   "metadata": {},
   "source": [
    "## Training with Custom Reward Function\n",
    "We will first load the default reward config, and your task is to tune them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ecf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import config_dict\n",
    "\n",
    "def reward_config() -> config_dict.ConfigDict:\n",
    "  return config_dict.create(\n",
    "      ctrl_dt=0.02,\n",
    "      sim_dt=0.004,\n",
    "      episode_length=1000,\n",
    "      Kp=35.0,\n",
    "      Kd=0.5,\n",
    "      action_repeat=1,\n",
    "      action_scale=1.0,\n",
    "      history_len=1,\n",
    "      soft_joint_pos_limit_factor=0.95,\n",
    "      noise_config=config_dict.create(\n",
    "          level=1.0,\n",
    "          scales=config_dict.create(\n",
    "              joint_pos=0.03,\n",
    "              joint_vel=1.5,\n",
    "              gyro=0.2,\n",
    "              gravity=0.05,\n",
    "              linvel=0.1,\n",
    "          ),\n",
    "      ),\n",
    "      reward_config=config_dict.create(\n",
    "        ### ----- ADJUST SETTINGS BELOW ----- ###\n",
    "          scales=config_dict.create(\n",
    "              torso_height=-0.0,\n",
    "              # Tracking.\n",
    "              tracking_lin_vel=1.0,\n",
    "              tracking_ang_vel=0.5,\n",
    "\n",
    "              # Base reward.\n",
    "              lin_vel_z=-0.5,\n",
    "              ang_vel_xy=-0.05,\n",
    "              orientation=-5.0,\n",
    "              # Other.\n",
    "              dof_pos_limits=-1.0,\n",
    "              pose=0.5,\n",
    "              # Other.\n",
    "              termination=-1.0,\n",
    "              stand_still=-1.0,\n",
    "              # Regularization.\n",
    "              torques=-0.0002,\n",
    "              action_rate=-0.01,\n",
    "              energy=-0.001,\n",
    "              # Feet.\n",
    "              feet_clearance=-0.2,\n",
    "              feet_slip=-0.1,\n",
    "              feet_air_time=0.1,\n",
    "          ),\n",
    "          tracking_sigma=0.25,\n",
    "          max_foot_height=0.11,        \n",
    "          desired_foot_air_time=0.15,\n",
    "          desired_torso_height=0.36\n",
    "        ### ----- ADJUST SETTINGS ABOVE ----- ###\n",
    "      ),\n",
    "      pert_config=config_dict.create(\n",
    "          enable=False,\n",
    "          velocity_kick=[0.0, 3.0],\n",
    "          kick_durations=[0.05, 0.2],\n",
    "          kick_wait_times=[1.0, 3.0],\n",
    "      ),\n",
    "      command_config=config_dict.create(\n",
    "          # Uniform distribution for command amplitude.\n",
    "          a=[1.5, 0.8, 1.2],\n",
    "          # Probability of not zeroing out new command.\n",
    "          b=[0.9, 0.25, 0.5],\n",
    "      ),\n",
    "      impl=\"jax\",\n",
    "      nconmax=4 * 8192,\n",
    "      njmax=40,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8094d59",
   "metadata": {},
   "source": [
    "You can reduce the final training time by configuring less frequent evaluations (and thus less frequent video generations), but to get started it's helpful to get a lot of feedback.\n",
    "\n",
    "You can also reduce the total training time if you feel like you have a good policy after few steps. The default is 200.000.000 steps of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_name = 'Go1JoystickRoughTerrain'\n",
    "env = registry.load(env_name, config=reward_config())\n",
    "key = jax.random.PRNGKey(15)\n",
    "\n",
    "randomizer = registry.get_domain_randomizer(env_name)\n",
    "print(f\"Environment '{env_name}' loaded successfully.\")\n",
    "\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "current_policy = None\n",
    "\n",
    "env_cfg = reward_config()\n",
    "\n",
    "eval_env_for_video = registry.load(env_name, config=env_cfg)\n",
    "jit_reset = jax.jit(eval_env_for_video.reset)\n",
    "jit_step = jax.jit(eval_env_for_video.step)\n",
    "\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    times.append(datetime.now())\n",
    "    x_data.append(num_steps)\n",
    "    y_data.append(metrics[\"eval/episode_reward\"])\n",
    "    y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
    "\n",
    "    plt.xlim([0, ppo_params[\"num_timesteps\"] * 1.25])\n",
    "    plt.xlabel(\"# environment steps\")\n",
    "    plt.ylabel(\"reward per episode\")\n",
    "    plt.title(f\"Challenging Terrain Training: reward={y_data[-1]:.3f}\")\n",
    "    plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"red\")\n",
    "    \n",
    "    display(plt.gcf())\n",
    "\n",
    "    # Render video if we have a current policy\n",
    "    if current_policy is not None:\n",
    "        render_video_during_training(current_policy, num_steps, jit_step, jit_reset, env_cfg, eval_env_for_video)\n",
    "\n",
    "# PPO training parameters - you may want to tune these for challenging terrain\n",
    "ppo_params = locomotion_params.brax_ppo_config(env_name)\n",
    "ppo_training_params = dict(ppo_params)\n",
    " \n",
    "ppo_training_params[\"num_evals\"] = 25 # Reduce for final training for less feedback.\n",
    "ppo_training_params[\"num_timesteps\"] = 200000000  # Total number of training steps\n",
    "\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "\n",
    "if \"network_factory\" in ppo_params:\n",
    "    del ppo_training_params[\"network_factory\"]\n",
    "    network_factory = functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    )\n",
    "\n",
    "print(\"Training parameters:\")\n",
    "print(ppo_training_params)\n",
    "\n",
    "# Create a policy parameters callback to capture the current policy\n",
    "def policy_params_callback(_, make_policy_fn, params):\n",
    "    global current_policy\n",
    "    current_policy = make_policy_fn(params, deterministic=True)\n",
    "    \n",
    "train_fn = functools.partial(\n",
    "        custom_ppo_train.train,\n",
    "        **ppo_training_params,\n",
    "        network_factory=network_factory,\n",
    "        randomization_fn=randomizer,\n",
    "        progress_fn=progress,\n",
    "        policy_params_fn=policy_params_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "start_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training on challenging terrain...\")\n",
    "make_policy, params, _ = train_fn(environment=env,\n",
    "                                  eval_env=registry.load(env_name, config=env_cfg),\n",
    "                                  wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "                                  compute_custom_ppo_loss_fn=ppo_losses.compute_ppo_loss\n",
    "                                 )\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_section",
   "metadata": {},
   "source": [
    "## Evaluate the Trained Policy\n",
    "\n",
    "Test your trained policy on the challenging terrain. You should see the robot successfully navigating steps and obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up evaluation environment with challenging conditions\n",
    "env_cfg = reward_config()\n",
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [2.0, 4.0]  # Add some perturbations\n",
    "env_cfg.pert_config.kick_wait_times = [3.0, 10.0]\n",
    "env_cfg.command_config.a = [1.2, 0.6, 2*jp.pi]  # Adjust command ranges\n",
    "\n",
    "eval_env = registry.load(env_name, config=env_cfg)\n",
    "velocity_kick_range = [0.0, 0.0]  # Disable velocity kick for clearer evaluation\n",
    "kick_duration_range = [0.05, 0.2]\n",
    "\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "jit_inference_fn = jax.jit(make_policy(params, deterministic=True))\n",
    "\n",
    "print(\"Evaluating policy on challenging terrain...\")\n",
    "evaluate_policy(\n",
    "    eval_env,\n",
    "    jit_inference_fn,\n",
    "    jit_step,\n",
    "    jit_reset,\n",
    "    env_cfg,\n",
    "    eval_env,\n",
    "    velocity_kick_range,\n",
    "    kick_duration_range,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
