{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e4f92d",
   "metadata": {},
   "source": [
    "# Lab 1 - Level E: PPO Loss Function Implementation\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this level, you will implement the missing components of the PPO (Proximal Policy Optimization) loss function and test it by training a quadruped robot to follow simple velocity commands.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Core concepts of the PPO algorithm\n",
    "- How to implement policy gradient loss functions\n",
    "- Working with JAX and MuJoCo simulation environments\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, make sure you have completed the setup instructions in the main `README.md` file. You should have:\n",
    "- Set up your Python virtual environment\n",
    "- Installed all required dependencies\n",
    "- Familiar with basic reinforcement learning concepts\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "Your main task is to complete the `compute_custom_ppo_loss` function. This function implements the core PPO algorithm components:\n",
    "\n",
    "1. **Policy Loss**: The clipped surrogate objective that prevents large policy updates\n",
    "2. **Value Function Loss**: Mean squared error between predicted and target values  \n",
    "3. **Entropy Loss**: Regularization term to encourage exploration\n",
    "\n",
    "The PPO algorithm uses a clipped objective to ensure stable training by preventing policy updates that are too large. You'll implement the mathematical formulation from the original PPO paper.\n",
    "\n",
    "## Expected Outcome\n",
    "\n",
    "After successful implementation, you should see:\n",
    "- The robot learning to follow velocity commands\n",
    "- Training progress with increasing reward over time\n",
    "- Video outputs showing the robot's locomotion behavior\n",
    "\n",
    "## Code Structure\n",
    "\n",
    "The notebook contains:\n",
    "- Environment setup and visualization\n",
    "- The PPO loss function template (your main task)\n",
    "- Training loop and progress tracking\n",
    "- Evaluation and video generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f2a04",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unset LD_LIBRARY_PATH\n",
    "import os\n",
    "if 'LD_LIBRARY_PATH' in os.environ:\n",
    "    os.environ['LD_LIBRARY_PATH'] = ''\n",
    "    print(\"LD_LIBRARY_PATH has been unset\")\n",
    "else:\n",
    "    print(\"LD_LIBRARY_PATH was not set\")\n",
    "# Set EGL vendor directory to include user location\n",
    "user_vendor_dir = os.path.expanduser('~/.local/share/glvnd/egl_vendor.d')\n",
    "os.makedirs(user_vendor_dir, exist_ok=True)\n",
    "# Create ICD config\n",
    "icd_config = {\n",
    "    \"file_format_version\": \"1.0.0\",\n",
    "    \"ICD\": {\n",
    "        \"library_path\": \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "import json\n",
    "with open(f'{user_vendor_dir}/10_nvidia.json', 'w') as f:\n",
    "    json.dump(icd_config, f, indent=2)\n",
    "# Set environment variable\n",
    "current_dirs = os.environ.get('__EGL_VENDOR_LIBRARY_DIRS', '/usr/share/glvnd/egl_vendor.d')\n",
    "os.environ['__EGL_VENDOR_LIBRARY_DIRS'] = f'{user_vendor_dir}:{current_dirs}'\n",
    "# Set MuJoCo to use EGL\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "print('Setting environment variable to use GPU rendering:')\n",
    "%env MUJOCO_GL=egl\n",
    "# Tell XLA to use Triton GEMM\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import functools\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from IPython.display import HTML, clear_output\n",
    "import mujoco\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "import cv2\n",
    "import custom_ppo_train\n",
    "from utils import render_video_during_training, evaluate_policy\n",
    "import mediapy as media\n",
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "#os.environ['MUJOCO_GL'] = 'egl'\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True   # Show visual geoms\n",
    "scene_option.geomgroup[3] = False  # Hide collision geoms\n",
    "scene_option.geomgroup[5] = True   # Show sites (including height scanner visualization)\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Show contact points\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_RANGEFINDER] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f317c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JAX devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e74061",
   "metadata": {},
   "source": [
    "## Load the Environment\n",
    "We will load a flat terrain environment, and show the robot in different initial conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f573424",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Go1JoystickFlatTerrain'\n",
    "env = registry.load(env_name)\n",
    "key = jax.random.PRNGKey(15)\n",
    "\n",
    "num_resets = 20\n",
    "frames_per_reset = 30\n",
    "\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "rollout = []\n",
    "\n",
    "for reset_idx in range(num_resets):\n",
    "    key, reset_key = jax.random.split(key)\n",
    "    \n",
    "    # Reset to new random position\n",
    "    state = jit_reset(reset_key)\n",
    "\n",
    "    robot_pos = state.data.qpos[:3]\n",
    "    \n",
    "    # Add multiple frames of the same reset position\n",
    "    for frame_idx in range(frames_per_reset):\n",
    "        rollout.append(state)\n",
    "        # Take a few small steps\n",
    "        if frame_idx < 5:\n",
    "            action = jp.zeros(env.action_size)\n",
    "            state = jit_step(state, action)\n",
    "\n",
    "\n",
    "render_every = 2  # Render every 2nd frame\n",
    "fps = 1.0 / env.dt / render_every\n",
    "traj = rollout[::render_every]\n",
    "\n",
    "frames = env.render(\n",
    "    traj,\n",
    "    camera=\"track\",  # Use tracking camera\n",
    "    scene_option=scene_option,\n",
    "    width=640,\n",
    "    height=480,\n",
    ")\n",
    "env_cfg = registry.get_default_config(env_name)\n",
    "eval_env_for_video = registry.load(env_name, config=env_cfg)\n",
    "jit_reset = jax.jit(eval_env_for_video.reset)\n",
    "jit_step = jax.jit(eval_env_for_video.step)\n",
    "media.show_video(frames, fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31048973",
   "metadata": {},
   "source": [
    "## Level E Implementation Task:\n",
    "\n",
    "**Your main task**: Complete the missing components in the `compute_custom_ppo_loss` function above.\n",
    "\n",
    "The function template is provided with most of the implementation, but you need to understand and verify the key PPO components:\n",
    "\n",
    "### Key Components to Understand:\n",
    "\n",
    "1. **Policy Loss (Clipped Surrogate Objective)**:\n",
    "   - Compute probability ratio: `rho_s = exp(new_log_prob - old_log_prob)`\n",
    "   - Apply clipping to prevent large policy updates\n",
    "   - Use minimum of clipped and unclipped objectives\n",
    "\n",
    "2. **Value Function Loss**:\n",
    "   - Mean squared error between predicted values and GAE targets\n",
    "   - Helps the critic learn to estimate state values accurately\n",
    "\n",
    "3. **Entropy Loss**:\n",
    "   - Regularization term to encourage exploration\n",
    "   - Prevents the policy from becoming too deterministic too early\n",
    "\n",
    "### Implementation Hints:\n",
    "\n",
    "- The function already includes the correct mathematical formulation\n",
    "- Pay attention to the clipping mechanism in the policy loss\n",
    "- Make sure you understand how advantages are normalized\n",
    "- The total loss combines all three components with appropriate weighting\n",
    "\n",
    "### Questions to Consider:\n",
    "\n",
    "1. Why does PPO use clipping instead of other policy gradient methods?\n",
    "2. What role does the advantage normalization play?\n",
    "3. How do the different loss components balance exploration vs exploitation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Any, Tuple\n",
    "from brax.training import types\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.types import Params\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from brax.training.agents.ppo.losses import compute_gae\n",
    "\n",
    "# Helper Struct for policy and value params\n",
    "@flax.struct.dataclass\n",
    "class PPONetworkParams:\n",
    "  \"\"\"Contains training state for the learner.\"\"\"\n",
    "\n",
    "  policy: Params\n",
    "  value: Params\n",
    "\n",
    "# PPO loss function to be implemented\n",
    "def compute_custom_ppo_loss(\n",
    "    params: PPONetworkParams,\n",
    "    normalizer_params: Any,\n",
    "    data: types.Transition,\n",
    "    rng: jnp.ndarray,\n",
    "    ppo_network: ppo_networks.PPONetworks,\n",
    "    entropy_cost: float = 1e-4,\n",
    "    discounting: float = 0.9,\n",
    "    reward_scaling: float = 1.0,\n",
    "    gae_lambda: float = 0.95,\n",
    "    clipping_epsilon: float = 0.3,\n",
    "    normalize_advantage: bool = True,\n",
    ") -> Tuple[jnp.ndarray, types.Metrics]:\n",
    "  \"\"\"Computes PPO loss.\n",
    "\n",
    "  Args:\n",
    "    params: Network parameters,\n",
    "    normalizer_params: Parameters of the normalizer.\n",
    "    data: Transition that with leading dimension [B, T]. extra fields required\n",
    "      are ['state_extras']['truncation'] ['policy_extras']['raw_action']\n",
    "      ['policy_extras']['log_prob']\n",
    "    rng: Random key\n",
    "    ppo_network: PPO networks.\n",
    "    entropy_cost: entropy cost.\n",
    "    discounting: discounting,\n",
    "    reward_scaling: reward multiplier.\n",
    "    gae_lambda: General advantage estimation lambda.\n",
    "    clipping_epsilon: Policy loss clipping epsilon\n",
    "    normalize_advantage: whether to normalize advantage estimate\n",
    "\n",
    "  Returns:\n",
    "    A tuple (loss, metrics)\n",
    "  \"\"\"\n",
    "  parametric_action_distribution = ppo_network.parametric_action_distribution\n",
    "  policy_apply = ppo_network.policy_network.apply\n",
    "  value_apply = ppo_network.value_network.apply\n",
    "\n",
    "  # Put the time dimension first.\n",
    "  data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)\n",
    "  policy_logits = policy_apply(\n",
    "      normalizer_params, params.policy, data.observation\n",
    "  )\n",
    "\n",
    "  baseline = value_apply(normalizer_params, params.value, data.observation)\n",
    "  terminal_obs = jax.tree_util.tree_map(lambda x: x[-1], data.next_observation)\n",
    "  bootstrap_value = value_apply(normalizer_params, params.value, terminal_obs)\n",
    "\n",
    "  rewards = data.reward * reward_scaling\n",
    "  truncation = data.extras['state_extras']['truncation']\n",
    "  termination = (1 - data.discount) * (1 - truncation)\n",
    "\n",
    "  target_action_log_probs = parametric_action_distribution.log_prob(\n",
    "      policy_logits, data.extras['policy_extras']['raw_action']\n",
    "  )\n",
    "  behaviour_action_log_probs = data.extras['policy_extras']['log_prob']\n",
    "\n",
    "  vs, advantages = compute_gae(\n",
    "      truncation=truncation,\n",
    "      termination=termination,\n",
    "      rewards=rewards,\n",
    "      values=baseline,\n",
    "      bootstrap_value=bootstrap_value,\n",
    "      lambda_=gae_lambda,\n",
    "      discount=discounting,\n",
    "  )\n",
    "  if normalize_advantage:\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "  \n",
    "  # TODO: Compute probability ratio between new and old policy\n",
    "  # Hint: Use jnp.exp() and compute the difference between log probabilities\n",
    "  rho_s = # YOUR CODE HERE\n",
    "  \n",
    "  # TODO: Implement the PPO clipped surrogate objective\n",
    "  # Hint: Compare unclipped (rho_s * advantages) vs clipped version\n",
    "  # Use jnp.clip() with clipping_epsilon parameter\n",
    "  surrogate_loss1 = # YOUR CODE HERE\n",
    "  surrogate_loss2 = # YOUR CODE HERE\n",
    "\n",
    "  # TODO: PPO policy loss is the negative mean of the minimum of both surrogates\n",
    "  policy_loss = # YOUR CODE HERE\n",
    "\n",
    "  # TODO: Implement value function loss\n",
    "  # Hint: Mean squared error between vs (targets) and baseline (predictions)\n",
    "  # Scale by 0.5 * 0.5 as in the original implementation\n",
    "  v_error = # YOUR CODE HERE\n",
    "  v_loss = # YOUR CODE HERE\n",
    "\n",
    "  # TODO: Implement entropy loss for exploration\n",
    "  # Hint: Use parametric_action_distribution.entropy() and multiply by entropy_cost\n",
    "  # Make it negative since we want to maximize entropy (minimize negative entropy)\n",
    "  entropy = # YOUR CODE HERE\n",
    "  entropy_loss = # YOUR CODE HERE\n",
    "\n",
    "  # TODO: Combine all loss components\n",
    "  total_loss = # YOUR CODE HERE\n",
    "  \n",
    "  return total_loss, {\n",
    "      'total_loss': total_loss,\n",
    "      'policy_loss': policy_loss,\n",
    "      'v_loss': v_loss,\n",
    "      'entropy_loss': entropy_loss,\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53383d54",
   "metadata": {},
   "source": [
    "You can reduce the final training time by configuring less frequent evaluations (and thus less frequent video generations), but to get started it's helpful to get a lot of feedback.\n",
    "\n",
    "You can also reduce the total training time if you feel like you have a good policy after few steps. The default is 200.000.000 steps of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb35d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cfg = registry.get_default_config(env_name)\n",
    "randomizer = registry.get_domain_randomizer(env_name)\n",
    "print(f\"Environment '{env_name}' loaded successfully.\")\n",
    "\n",
    "\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "# Store the current policy for video rendering\n",
    "current_policy = None\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    times.append(datetime.now())\n",
    "    x_data.append(num_steps)\n",
    "    y_data.append(metrics[\"eval/episode_reward\"])\n",
    "    y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
    "\n",
    "    plt.xlim([0, ppo_params[\"num_timesteps\"] * 1.25])\n",
    "    plt.xlabel(\"# environment steps\")\n",
    "    plt.ylabel(\"reward per episode\")\n",
    "    plt.title(f\"y={y_data[-1]:.3f}\")\n",
    "    plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n",
    "    \n",
    "    # Save the plot to a file instead of displaying it\n",
    "    display(plt.gcf())\n",
    "\n",
    "    # Render video if we have a current policy\n",
    "    if current_policy is not None:\n",
    "        render_video_during_training(current_policy, num_steps, jit_step, jit_reset, env_cfg, eval_env_for_video)\n",
    "\n",
    "\n",
    "ppo_params = locomotion_params.brax_ppo_config(env_name)\n",
    "ppo_training_params = dict(ppo_params)\n",
    "\n",
    "ppo_training_params[\"num_evals\"] = 25 # Reduce for final training for less feedback.\n",
    "ppo_training_params[\"num_timesteps\"] = 200000000  # Total number of training steps\n",
    " \n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "\n",
    "if \"network_factory\" in ppo_params:\n",
    "    del ppo_training_params[\"network_factory\"]\n",
    "    network_factory = functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    )\n",
    "print(ppo_training_params)\n",
    "\n",
    "# Create a policy parameters callback to capture the current policy\n",
    "def policy_params_callback(_, make_policy_fn, params):\n",
    "    # Update the current policy for video rendering\n",
    "    global current_policy\n",
    "    current_policy = make_policy_fn(params, deterministic=True)\n",
    "    \n",
    "train_fn = functools.partial(\n",
    "        custom_ppo_train.train,\n",
    "        **ppo_training_params,\n",
    "        network_factory=network_factory,\n",
    "        randomization_fn=randomizer,\n",
    "        progress_fn=progress,\n",
    "        policy_params_fn=policy_params_callback,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbaa78a",
   "metadata": {},
   "source": [
    "Run the training. The first steps should take a while, since we needto wait for jax to compile the training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "make_policy, params, _ = train_fn(environment=env,\n",
    "                                  eval_env=registry.load(env_name, config=env_cfg),\n",
    "                                  wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "                                  compute_custom_ppo_loss_fn=compute_custom_ppo_loss\n",
    "                                 )\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b921efe",
   "metadata": {},
   "source": [
    "## Evaluate the trained model\n",
    "You should see the robot following the command sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627195b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cfg = registry.get_default_config(env_name)\n",
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [3.0, 6.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2*jp.pi]\n",
    "eval_env = registry.load(env_name, config=env_cfg)\n",
    "velocity_kick_range = [0.0, 0.0]  # Disable velocity kick.\n",
    "kick_duration_range = [0.05, 0.2]\n",
    "\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "jit_inference_fn = jax.jit(make_policy(params, deterministic=True))\n",
    "\n",
    "evaluate_policy(\n",
    "    eval_env,\n",
    "    jit_inference_fn,\n",
    "    jit_step,\n",
    "    jit_reset,\n",
    "    env_cfg,\n",
    "    eval_env,\n",
    "    velocity_kick_range,\n",
    "    kick_duration_range,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
